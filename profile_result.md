# 性能分析报告

以下为PyTorch Profiler抓取的Transformer模型进行训练操作的性能分析。可见GPU时间主要消耗在Addmm以及Sgemm算子（即全连接层以及Attention中的matmul）。
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
                                        model_inference        11.12%      76.221ms        99.45%     681.497ms     681.497ms       0.000us         0.00%     568.246ms     568.246ms      27.64 Kb    -418.35 Kb       1.31 Gb     -34.95 Gb             1
                                           aten::linear         1.45%       9.945ms        16.53%     113.294ms      72.999us       0.000us         0.00%     377.298ms     243.104us           0 b           0 b      10.41 Gb     344.24 Mb          1552
                                            aten::addmm         6.61%      45.277ms        12.85%      88.049ms      57.324us     278.367ms        55.19%     305.112ms     198.641us           0 b           0 b       7.28 Gb       5.75 Gb          1536
                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     253.247ms        50.21%     253.247ms     172.865us           0 b           0 b           0 b           0 b          1465
                                           aten::matmul         0.89%       6.094ms         8.36%      57.266ms      96.733us       0.000us         0.00%     126.028ms     212.885us           0 b           0 b       7.99 Gb           0 b           592
                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     107.037ms        21.22%     107.037ms       1.230ms           0 b           0 b           0 b           0 b            87
                                               aten::mm         0.08%     521.000us         0.17%       1.176ms      73.500us      82.072ms        16.27%      82.256ms       5.141ms           0 b           0 b       3.13 Gb       3.13 Gb            16
                                            aten::copy_         1.51%      10.365ms        25.08%     171.830ms     108.479us      30.825ms         6.11%      36.053ms      22.761us      62.50 Kb    -415.50 Kb           0 b           0 b          1584
                                            aten::clone         0.83%       5.714ms         5.44%      37.290ms      25.316us       0.000us         0.00%      35.460ms      24.073us     450.75 Kb           0 b       4.86 Gb    -163.48 Mb          1473
                                       cudaLaunchKernel        18.76%     128.531ms        18.76%     128.531ms      13.755us      34.484ms         6.84%      34.484ms       3.690us           0 b           0 b      -1.00 Mb      -1.00 Mb          9344
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 685.260ms
Self CUDA time total: 504.416ms
```
以下为PyTorch Profiler抓取的Transformer模型进行推理操作的性能分析。同样，合计80%的GPU时间都消耗在Addmm以及Sgemm算子（即全连接层以及Attention中的matmul）。
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                        model_inference        22.24%        1.262s       100.00%        5.674s        5.674s       0.000us         0.00%        2.413s        2.413s           0 b      -2.12 Kb           0 b    -116.16 Gb             1  
                                           aten::linear         3.17%     179.786ms        23.74%        1.347s      43.158us       0.000us         0.00%        1.412s      45.244us           0 b           0 b      31.59 Gb       1.23 Gb         31207  
                                            aten::addmm        11.58%     656.965ms        17.52%     994.041ms      32.383us     992.857ms        45.94%        1.069s      34.810us           0 b           0 b      16.93 Gb     -13.05 Gb         30696  
                                  volta_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     735.565ms        34.03%     735.565ms      55.372us           0 b           0 b           0 b           0 b         13284  
                                           aten::matmul         1.99%     112.927ms         8.84%     501.285ms      39.203us       0.000us         0.00%     602.056ms      47.083us           0 b           0 b      26.23 Gb           0 b         12787  
                                               aten::mm         0.18%      10.101ms         0.24%      13.560ms      26.536us     373.793ms        17.30%     374.909ms     733.677us           0 b           0 b      14.66 Gb      14.66 Gb           511  
                                 volta_sgemm_128x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us     281.027ms        13.00%     281.027ms     197.767us           0 b           0 b           0 b           0 b          1421  
                                              aten::bmm         3.13%     177.712ms         4.36%     247.241ms      20.140us     203.903ms         9.43%     227.147ms      18.503us           0 b           0 b      11.57 Gb      11.57 Gb         12276  
                                       cudaLaunchKernel        13.36%     757.979ms        13.36%     757.979ms       4.366us     176.555ms         8.17%     176.781ms       1.018us           0 b           0 b      -2.00 Mb      -2.00 Mb        173617  
                                   volta_sgemm_64x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     138.677ms         6.42%     138.677ms      20.936us           0 b           0 b           0 b           0 b          6624  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 5.674s
Self CUDA time total: 2.161s
```
由以上数据可见，需要优化的算子为Attention算子，以及前后出现的全连接层。具体的优化措施：参考PyTorch的底层实现，并自行融合相关算子。
